{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8283529,
          "sourceType": "datasetVersion",
          "datasetId": 4802681
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "KABYTE_test",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate requests ultralytics supervision"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-03T07:04:59.847279Z",
          "iopub.execute_input": "2024-05-03T07:04:59.847659Z",
          "iopub.status.idle": "2024-05-03T07:05:15.259947Z",
          "shell.execute_reply.started": "2024-05-03T07:04:59.847626Z",
          "shell.execute_reply": "2024-05-03T07:05:15.259017Z"
        },
        "trusted": true,
        "id": "uMKeIUXoOiQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T07:05:59.082607Z",
          "iopub.execute_input": "2024-05-03T07:05:59.082998Z",
          "iopub.status.idle": "2024-05-03T07:06:38.962062Z",
          "shell.execute_reply.started": "2024-05-03T07:05:59.082966Z",
          "shell.execute_reply": "2024-05-03T07:06:38.960971Z"
        },
        "trusted": true,
        "id": "4zMBf2gHOiQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import supervision as sv\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from transformers import (\n",
        "    AutoImageProcessor, AutoModelForImageClassification,\n",
        "    AutoModelForObjectDetection, ImageClassificationPipeline,\n",
        "    TrainingArguments, Trainer, pipeline\n",
        ")\n",
        "\n",
        "from transformers.image_utils import load_image\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageDraw\n",
        "import cv2\n",
        "\n",
        "from requests import get\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "from supervision import Detections\n",
        "\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# skimage\n",
        "from skimage.io import imshow, imread, imsave\n",
        "from skimage.transform import rotate, AffineTransform, warp,rescale, resize, downscale_local_mean\n",
        "from skimage import color,data\n",
        "from skimage.exposure import adjust_gamma\n",
        "from skimage.util import random_noise"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:16:14.179289Z",
          "iopub.execute_input": "2024-05-03T08:16:14.179775Z",
          "iopub.status.idle": "2024-05-03T08:16:14.194767Z",
          "shell.execute_reply.started": "2024-05-03T08:16:14.179739Z",
          "shell.execute_reply": "2024-05-03T08:16:14.193742Z"
        },
        "trusted": true,
        "id": "_Ywi4uRXOiQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aug Data Reading"
      ],
      "metadata": {
        "id": "7CrlSbWzOiQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Словарь для преобразования числовых меток в строковые\n",
        "label_mapping = {\n",
        "    0: \"neutral\",\n",
        "    1: \"microsleep\",\n",
        "    2: \"yawning\",\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "id": "SXz8R7TKOiQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing the images\n",
        "directory = '/kaggle/input/aminfins/gaz_aug_zip/neutral_aug/'\n",
        "\n",
        "# Create a list to store PIL images\n",
        "neutral_images_new = []\n",
        "\n",
        "# Iterate over all files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "        # Construct the full path to the image file\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        # Open the image file as a PIL image and append it to the list\n",
        "        pil_image = Image.open(file_path)\n",
        "        neutral_images_new.append(pil_image)"
      ],
      "metadata": {
        "trusted": true,
        "id": "fUGR2BDHOiQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing the images\n",
        "directory = '/kaggle/input/aminfins/gaz_aug_zip/microsleep_aug/'\n",
        "\n",
        "# Create a list to store PIL images\n",
        "microsleep_images_new = []\n",
        "\n",
        "# Iterate over all files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "        # Construct the full path to the image file\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        # Open the image file as a PIL image and append it to the list\n",
        "        pil_image = Image.open(file_path)\n",
        "        microsleep_images_new.append(pil_image)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ROrIxUXkOiQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing the images\n",
        "directory = '/kaggle/input/aminfins/gaz_aug_zip/yawning_aug/'\n",
        "\n",
        "# Create a list to store PIL images\n",
        "yawning_images_new = []\n",
        "\n",
        "# Iterate over all files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "        # Construct the full path to the image file\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        # Open the image file as a PIL image and append it to the list\n",
        "        pil_image = Image.open(file_path)\n",
        "        yawning_images_new.append(pil_image)"
      ],
      "metadata": {
        "trusted": true,
        "id": "KlTw7CKpOiQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neutral_labels = [0] * len(neutral_images_new)\n",
        "microsleep_labels = [1] * len(microsleep_images_new)\n",
        "yawning_labels = [2] * len(yawning_images_new)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_DHM_swGOiQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_images = neutral_images_new + microsleep_images_new + yawning_images_new\n",
        "all_labels = neutral_labels + microsleep_labels + yawning_labels"
      ],
      "metadata": {
        "trusted": true,
        "id": "pIK2YGF4OiQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение данных на тренировочную и тестовую выборку\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    all_images, all_labels, test_size=0.15, random_state=0)\n",
        "\n",
        "# Создание объектов Dataset для тренировочной и тестовой выборок\n",
        "train_dataset = Dataset.from_dict({\"image\": train_images, \"label\": train_labels})\n",
        "test_dataset = Dataset.from_dict({\"image\": test_images, \"label\": test_labels})\n",
        "\n",
        "# Создание объекта DatasetDict\n",
        "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "# Вывод информации о DatasetDict\n",
        "print(dataset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1RR4_pnfOiQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"accuracy\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "bIJwv8cvOiQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset[\"train\"][122]\n",
        "print(label_mapping[example['label']])\n",
        "example['image']"
      ],
      "metadata": {
        "trusted": true,
        "id": "AFdp7485OiQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = label_mapping\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "model_checkpoint = \"microsoft/resnet-50\"\n",
        "batch_size = 8\n",
        "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AhachLDYOiQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "if \"height\" in image_processor.size:\n",
        "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        "    crop_size = size\n",
        "    max_size = None\n",
        "elif \"shortest_edge\" in image_processor.size:\n",
        "    size = image_processor.size[\"shortest_edge\"]\n",
        "    crop_size = (size, size)\n",
        "    max_size = image_processor.size.get(\"longest_edge\")\n",
        "\n",
        "train_transforms = Compose(\n",
        "        [\n",
        "            RandomResizedCrop(crop_size),\n",
        "            RandomHorizontalFlip(),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "val_transforms = Compose(\n",
        "        [\n",
        "            Resize(size),\n",
        "            CenterCrop(crop_size),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def preprocess_train(example_batch):\n",
        "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "\n",
        "def preprocess_val(example_batch):\n",
        "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
        "    return example_batch"
      ],
      "metadata": {
        "trusted": true,
        "id": "ht8A0VvoOiQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset['train']\n",
        "val_ds = dataset['test']\n",
        "\n",
        "train_ds.set_transform(preprocess_train)\n",
        "val_ds.set_transform(preprocess_val)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ox917MbIOiQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "vDwUs1r2OiQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes = True,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ],
      "metadata": {
        "trusted": true,
        "id": "loDSXK4MOiQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    \"train_image_kabyte\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    num_train_epochs=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "xk2swi94OiRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = trainer.train()\n",
        "# rest is optional but nice to have\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "trusted": true,
        "id": "VjUWKTZwOiRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "CaUiwQXkOiRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels = []\n",
        "classifier = pipeline(\"image-classification\", model='/kaggle/working/train_image_kabyte/checkpoint-3110')\n",
        "\n",
        "for image in test_images:\n",
        "    pred_label = label2id[classifier(image)[0]['label']]\n",
        "    pred_labels.append(pred_label)"
      ],
      "metadata": {
        "trusted": true,
        "id": "QMI3LVY_OiRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = pd.DataFrame({\n",
        "    \"Test\": [\n",
        "        accuracy_score(pred_labels, test_labels),\n",
        "        precision_score(pred_labels, test_labels, average='macro'),\n",
        "        recall_score(pred_labels, test_labels, average='macro'),\n",
        "        f1_score(pred_labels, test_labels, average='macro'),\n",
        "    ],\n",
        "}, index = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
        "\n",
        "metrics"
      ],
      "metadata": {
        "trusted": true,
        "id": "HFDQ6RoUOiRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 epochs\n",
        "![image.png](attachment:739b616a-bd24-4353-a682-227f974c44ac.png)\n"
      ],
      "metadata": {
        "id": "oEgxyx7qOiRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 epochs\n",
        "![image.png](attachment:1260e84f-2d0d-47e8-84c7-4f9dcba66061.png)"
      ],
      "metadata": {
        "id": "FzCPskbqOiRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"image-classification\", model='/kaggle/input/aminfins/checkpoint-3110')"
      ],
      "metadata": {
        "trusted": true,
        "id": "8PlgVhApOiRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.shutterstock.com/image-photo/young-handsome-man-waking-morning-260nw-254734102.jpg'\n",
        "image = Image.open(get(url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "trusted": true,
        "id": "djVS4B4TOiRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier1500 = pipeline(\"image-classification\", model='/kaggle/input/aminfins/checkpoint-1500')\n",
        "classifier3110 = pipeline(\"image-classification\", model='/kaggle/input/aminfins/checkpoint-3110')\n",
        "\n",
        "print(f'1500: {classifier1500(image)[0]}')\n",
        "print(f'3110: {classifier3110(image)[0]}')"
      ],
      "metadata": {
        "trusted": true,
        "id": "KawezQH1OiRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier with Face Detection"
      ],
      "metadata": {
        "id": "quj5wSyLOiRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clf_face_3110(url):\n",
        "    # download model\n",
        "    model_path = hf_hub_download(repo_id=\"arnabdhar/YOLOv8-Face-Detection\", filename=\"model.pt\")\n",
        "\n",
        "    # load model\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # label mapping\n",
        "    label2text = {\n",
        "        \"neutral\": 'бодрствует',\n",
        "        \"microsleep\": 'спит',\n",
        "        \"yawning\": 'зевает',\n",
        "    }\n",
        "\n",
        "    # inference\n",
        "    output = model(Image.open(get(url, stream=True).raw))\n",
        "    results = Detections.from_ultralytics(output[0])\n",
        "\n",
        "    # Create a list to store cropped images\n",
        "    cropped_images = []\n",
        "\n",
        "    if results.xyxy.size > 0:\n",
        "\n",
        "        # Create figure and axes\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        # Display the image\n",
        "        image = Image.open(get(url, stream=True).raw)\n",
        "        ax.imshow(image)\n",
        "\n",
        "        # Draw boxes for detected faces with padding\n",
        "        for i, xyxy in enumerate(results.xyxy):\n",
        "            x_min, y_min, x_max, y_max = xyxy\n",
        "            padding = (y_max - y_min)*0.5\n",
        "            # Add padding\n",
        "            x_min -= padding\n",
        "            y_min -= padding\n",
        "            x_max += padding\n",
        "            y_max += 2.5*padding\n",
        "            rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "                                     linewidth=1, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # Annotate with numbers and background\n",
        "            bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"red\", lw=1)\n",
        "            ax.text(x_min, y_min, str(i+1), color='black', fontsize=12, ha='left', va='top', bbox=bbox_props)\n",
        "\n",
        "            # Crop the image based on the bounding box\n",
        "            cropped_img = image.crop((x_min, y_min, x_max, y_max))\n",
        "            cropped_images.append(cropped_img)\n",
        "        plt.show()\n",
        "\n",
        "        # Classifier model\n",
        "        classifier = pipeline(\"image-classification\", model='/kaggle/input/aminfins/checkpoint-3110')\n",
        "\n",
        "        # Classify each cropped image\n",
        "        for i, cropped_img in enumerate(cropped_images):\n",
        "            print(f\"Человек на картинке {i+1} с вероятностью {int(100 * round(classifier(cropped_img)[0]['score'], 2))}% {label2text[classifier(cropped_img)[0]['label']]}.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Людей не фото не обнаружено.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T10:12:08.167773Z",
          "iopub.execute_input": "2024-05-03T10:12:08.168192Z",
          "iopub.status.idle": "2024-05-03T10:12:08.182305Z",
          "shell.execute_reply.started": "2024-05-03T10:12:08.168164Z",
          "shell.execute_reply": "2024-05-03T10:12:08.181313Z"
        },
        "trusted": true,
        "id": "z9IQ38szOiRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier with Person Detection"
      ],
      "metadata": {
        "id": "Ng33dW2KOiRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clf_person_3110(url):\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(get(url, stream=True).raw)\n",
        "\n",
        "    # Load processor and model\n",
        "    processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
        "    model = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
        "\n",
        "    # Process inputs\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Post-process object detection results\n",
        "    target_sizes = torch.tensor([image.size[::-1]])\n",
        "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.6)[0]\n",
        "\n",
        "    # label mapping\n",
        "    label2text = {\n",
        "        \"neutral\": 'бодрствует',\n",
        "        \"microsleep\": 'спит',\n",
        "        \"yawning\": 'зевает',\n",
        "    }\n",
        "\n",
        "    filtered_detections = [(score, label, box) for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]) if model.config.id2label[label.item()] == 'person' and score.item() > 0.88]\n",
        "\n",
        "    if filtered_detections:\n",
        "\n",
        "        # Create figure and axes\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        # Display the image\n",
        "        ax.imshow(image)\n",
        "\n",
        "        # Store cropped images of persons\n",
        "        cropped_images = []\n",
        "\n",
        "        # Iterate over the results, only for persons with high confidence\n",
        "        for i, (score, label, box) in enumerate(filtered_detections):\n",
        "            box = [round(coord, 2) for coord in box.tolist()]\n",
        "            # Draw bounding box on the image\n",
        "            rect = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # Annotate with numbers and background\n",
        "            bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"red\", lw=1)\n",
        "            ax.text(box[0], box[1], str(i+1), color='black', fontsize=12, ha='left', va='top', bbox=bbox_props)\n",
        "\n",
        "            # Crop the person from the original image\n",
        "            person_img = image.crop((box[0], box[1], box[2], box[3]))  # Corrected cropping coordinates\n",
        "            cropped_images.append(person_img)\n",
        "\n",
        "        # Show the image with bounding boxes\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        # Classifier model\n",
        "        classifier = pipeline(\"image-classification\", model='/kaggle/input/aminfins/checkpoint-3110')\n",
        "\n",
        "        # Classify each cropped image\n",
        "        for i, cropped_img in enumerate(cropped_images):\n",
        "            # Classify the cropped image\n",
        "            prediction = classifier(cropped_img)[0]\n",
        "            # Get the label and score\n",
        "            label_text = label2text[prediction['label']]\n",
        "            score = int(100 * round(prediction['score'], 2))\n",
        "            # Print the classification result\n",
        "            print(f\"Человек на картинке {i+1} с вероятностью {score}% {label_text}.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Людей не фото не обнаружено.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T10:29:55.08167Z",
          "iopub.execute_input": "2024-05-03T10:29:55.082071Z",
          "iopub.status.idle": "2024-05-03T10:29:55.098352Z",
          "shell.execute_reply.started": "2024-05-03T10:29:55.082045Z",
          "shell.execute_reply": "2024-05-03T10:29:55.097373Z"
        },
        "trusted": true,
        "id": "5M-BhOCCOiRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "'https://as1.ftcdn.net/v2/jpg/00/72/21/90/1000_F_72219046_WmTgchiYSNEUWrGggispWfYdT2xLDhbZ.jpg',\n",
        "'https://t3.ftcdn.net/jpg/02/51/45/80/240_F_251458070_cbrSmk8NqXWJpxEzxWbLpIJE8kCnFL5i.jpg',\n",
        "'https://as1.ftcdn.net/v2/jpg/01/13/52/06/1000_F_113520656_sztSZqJPTU2ULEwXZGJ2Dw5zh0B3lueU.jpg',\n",
        "'https://as1.ftcdn.net/v2/jpg/03/32/93/82/1000_F_332938226_oyspt8Cdk0F4C9wcOPgdq5oN08z62zqg.jpg',\n",
        "'https://as2.ftcdn.net/v2/jpg/03/32/43/51/1000_F_332435143_xvOYyIrMCKySKnwAZxN29BSNV63ROrHB.jpg',\n",
        "'https://as1.ftcdn.net/v2/jpg/06/91/69/06/1000_F_691690648_lgkgUeLpBdKwjzs3L7SgvQgqjit4Ve1j.jpg',\n",
        "'https://as2.ftcdn.net/v2/jpg/06/91/69/11/1000_F_691691195_6ZdSJKRFy3IPyuvXeTJ65OAcc76LSX76.jpg',\n",
        "'https://as2.ftcdn.net/v2/jpg/03/25/85/63/1000_F_325856361_gJqkgVbfVDZyV0NL5vmPFNTovwv7rtJN.jpg',\n",
        "'https://as2.ftcdn.net/v2/jpg/02/74/41/35/1000_F_274413590_2XDO1TnFvjNmq5zuGxKbbyHbFqFPQjoI.jpg',\n",
        "'https://as1.ftcdn.net/v2/jpg/07/80/42/00/1000_F_780420090_KqPbLs5bQt5kGlpyVd3emVV8v3Iq766c.jpg',\n",
        "'https://as2.ftcdn.net/v2/jpg/06/81/01/79/1000_F_681017952_uAyml8YJ7rr4ztyr6b2OGoGo9Ye67S4F.jpg',\n",
        "'https://as2.ftcdn.net/v2/jpg/06/80/85/63/1000_F_680856330_ZJ4qhDG2R7E5FC79Xwego7nv0nI9guPm.jpg',\n",
        "'https://as2.ftcdn.net/v2/jpg/05/69/86/83/1000_F_569868386_6MpAX89OkxOCRI7vyP45r84EncqSRjIn.jpg',\n",
        "'https://as1.ftcdn.net/v2/jpg/03/86/17/72/1000_F_386177263_0lBUHkxb5D6pr5a0vsr4E58u6wdCkcPb.jpg',\n",
        "'https://as2.ftcdn.net/v2/jpg/02/84/52/99/1000_F_284529974_4gQdVf3pGjhWEdS5FmlaRdmfsXcwaa3U.jpg',\n",
        "'https://as1.ftcdn.net/v2/jpg/05/17/96/20/1000_F_517962042_PssY3rZpOEpC9LN0kiF3t5m3vdn18LXd.jpg'\n",
        "]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T10:39:52.541412Z",
          "iopub.execute_input": "2024-05-03T10:39:52.541937Z",
          "iopub.status.idle": "2024-05-03T10:39:52.549817Z",
          "shell.execute_reply.started": "2024-05-03T10:39:52.541899Z",
          "shell.execute_reply": "2024-05-03T10:39:52.54858Z"
        },
        "trusted": true,
        "id": "Mrnpwry0OiRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "clf_person_3110(urls[-1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T10:39:53.514744Z",
          "iopub.execute_input": "2024-05-03T10:39:53.515555Z",
          "iopub.status.idle": "2024-05-03T10:39:56.841371Z",
          "shell.execute_reply.started": "2024-05-03T10:39:53.515526Z",
          "shell.execute_reply": "2024-05-03T10:39:56.840319Z"
        },
        "trusted": true,
        "id": "2hYbKTbZOiRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Заметки:\n",
        "1. если модель видит в кадре руку рядом с лицом, она с большой вероятностью предскажет зевание\n",
        "2. если у человека закрыты глаза, например, он моргнул, прищурился или опустил их, модель с большой вероятностью предскажет сон\n",
        "3. если картинка размазана, она с большой вероятностью предскажет сон, так как на лице будет больше темных пикселей\n",
        "4. Использование полигонов вместо прямоугольных рамок возможно улучшило бы модель.\n",
        "5. Если на картинке всего один человек, модель справляется очень хорошо\n",
        "6. Рамки могут перекрываться, что может запутать модель\n",
        "7. Более разнообразные данные (люди с книгами, кружками, ноутбуками и т.п.) также улучшили бы качество модели\n",
        "8. Модель с лицами работает быстрее, чем модель с человеком полностью, и уж тем более намного быстрее чем SAM."
      ],
      "metadata": {
        "id": "mb37O_9ZOiRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Segmentation"
      ],
      "metadata": {
        "id": "YzVkbafkOiRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_TYPE = \"vit_h\"\n",
        "\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Zhi6EUopOiRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image from URL\n",
        "url = 'https://as1.ftcdn.net/v2/jpg/00/72/21/90/1000_F_72219046_WmTgchiYSNEUWrGggispWfYdT2xLDhbZ.jpg'\n",
        "image = Image.open(get(url, stream=True).raw)\n",
        "image_array = np.array(image)\n",
        "image"
      ],
      "metadata": {
        "trusted": true,
        "id": "HNOSy36eOiRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam_result = mask_generator.generate(image_array)"
      ],
      "metadata": {
        "trusted": true,
        "id": "95HLisqeOiRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
        "\n",
        "annotated_image = mask_annotator.annotate(scene=image_array.copy(), detections=detections)\n",
        "\n",
        "if image_array.shape[-1] == 3:\n",
        "    image_rgb = image_array\n",
        "else:\n",
        "    image_rgb = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Convert the image to a PIL Image object\n",
        "image_pil = Image.fromarray(image_rgb)\n",
        "\n",
        "# Now you can visualize the results\n",
        "sv.plot_images_grid(\n",
        "    images=[image_pil, annotated_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "AI_CK1ogOiRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks = [\n",
        "    mask['segmentation']\n",
        "    for mask\n",
        "    in sorted(sam_result, key=lambda x: x['area'], reverse=True)\n",
        "]\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=masks,\n",
        "    grid_size=(10, 10),\n",
        "    size=(16, 16)\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9x-cRRbbOiRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter detections to keep only those related to humans\n",
        "human_detections = [detection for detection in detections if detection.label == 'person']\n",
        "\n",
        "# If there are no detections related to humans, handle the case accordingly\n",
        "if not human_detections:\n",
        "    print(\"No human detections found.\")\n",
        "    # You can choose to exit the code or perform some other action\n",
        "else:\n",
        "    # Extract mask data for human detections\n",
        "    human_mask_data = sam_result.masks_for_detections(human_detections)\n",
        "\n",
        "    # Annotate the image with human masks\n",
        "    human_annotated_image = mask_annotator.annotate(scene=image_array.copy(), detections=human_detections, mask_data=human_mask_data)\n",
        "\n",
        "    # Convert the image to a PIL Image object\n",
        "    if image_array.shape[-1] == 3:\n",
        "        image_rgb = image_array\n",
        "    else:\n",
        "        image_rgb = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
        "    image_pil = Image.fromarray(image_rgb)\n",
        "\n",
        "    # Visualize the results\n",
        "    sv.plot_images_grid(\n",
        "        images=[image_pil, human_annotated_image],\n",
        "        grid_size=(1, 2),\n",
        "        titles=['source image', 'segmented image (humans only)']\n",
        "    )\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "E-KYFvCyOiRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Another Try with Boxes"
      ],
      "metadata": {
        "id": "K4B4tBNpOiRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://as1.ftcdn.net/v2/jpg/00/72/21/90/1000_F_72219046_WmTgchiYSNEUWrGggispWfYdT2xLDhbZ.jpg'\n",
        "image = Image.open(get(url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T07:09:36.161941Z",
          "iopub.execute_input": "2024-05-03T07:09:36.162323Z",
          "iopub.status.idle": "2024-05-03T07:09:36.783193Z",
          "shell.execute_reply.started": "2024-05-03T07:09:36.162295Z",
          "shell.execute_reply": "2024-05-03T07:09:36.782211Z"
        },
        "trusted": true,
        "id": "7-5Uq7cfOiRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image from URL\n",
        "url = 'https://as1.ftcdn.net/v2/jpg/06/91/69/06/1000_F_691690648_lgkgUeLpBdKwjzs3L7SgvQgqjit4Ve1j.jpg'\n",
        "response = requests.get(url)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Load processor and model\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
        "model = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
        "\n",
        "# Process inputs\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Post-process object detection results\n",
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.4)[0]\n",
        "\n",
        "# Store cropped images of persons\n",
        "person_images = []\n",
        "boxes = []\n",
        "\n",
        "# Create figure and axes\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Display the image\n",
        "ax.imshow(image)\n",
        "\n",
        "# Draw boxes only for persons with high confidence\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    label_name = model.config.id2label[label.item()]\n",
        "    if label_name == 'person' and score.item() > 0.88:  # Adjust the threshold here\n",
        "        color = (1, 0, 0)  # Red color\n",
        "        rect = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=1, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # Crop the person from the original image\n",
        "        person_img = image.crop((box[0], box[1], box[2], box[3]))  # Corrected cropping coordinates\n",
        "        person_images.append(person_img)\n",
        "        boxes.append(box)\n",
        "\n",
        "image_np = np.array(image)\n",
        "\n",
        "# Show the image with bounding boxes\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T07:59:28.093122Z",
          "iopub.execute_input": "2024-05-03T07:59:28.093789Z",
          "iopub.status.idle": "2024-05-03T07:59:31.653358Z",
          "shell.execute_reply.started": "2024-05-03T07:59:28.09376Z",
          "shell.execute_reply": "2024-05-03T07:59:31.65238Z"
        },
        "trusted": true,
        "id": "TOLwO6kJOiRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boxes[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T07:58:13.563921Z",
          "iopub.execute_input": "2024-05-03T07:58:13.564583Z",
          "iopub.status.idle": "2024-05-03T07:58:13.570823Z",
          "shell.execute_reply.started": "2024-05-03T07:58:13.564549Z",
          "shell.execute_reply": "2024-05-03T07:58:13.569808Z"
        },
        "trusted": true,
        "id": "-_826o7EOiRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_TYPE = \"vit_h\"\n",
        "sam_checkpoint = \"/kaggle/working/sam_vit_h_4b8939.pth\"\n",
        "\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=sam_checkpoint).to(device=DEVICE)\n",
        "predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T07:47:01.417256Z",
          "iopub.execute_input": "2024-05-03T07:47:01.418184Z",
          "iopub.status.idle": "2024-05-03T07:47:08.696579Z",
          "shell.execute_reply.started": "2024-05-03T07:47:01.41815Z",
          "shell.execute_reply": "2024-05-03T07:47:08.695741Z"
        },
        "trusted": true,
        "id": "54oO_3vEOiRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.set_image(image_np)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T07:59:45.556388Z",
          "iopub.execute_input": "2024-05-03T07:59:45.557168Z",
          "iopub.status.idle": "2024-05-03T07:59:46.621422Z",
          "shell.execute_reply.started": "2024-05-03T07:59:45.557133Z",
          "shell.execute_reply": "2024-05-03T07:59:46.620433Z"
        },
        "trusted": true,
        "id": "oaMD5zYmOiRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T07:59:49.881855Z",
          "iopub.execute_input": "2024-05-03T07:59:49.882235Z",
          "iopub.status.idle": "2024-05-03T07:59:49.892535Z",
          "shell.execute_reply.started": "2024-05-03T07:59:49.882205Z",
          "shell.execute_reply": "2024-05-03T07:59:49.891641Z"
        },
        "trusted": true,
        "id": "sq59BJ91OiRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_box = np.array(box)\n",
        "\n",
        "masks, _, _ = predictor.predict(\n",
        "    point_coords=None,\n",
        "    point_labels=None,\n",
        "    box=input_box[None, :],\n",
        "    multimask_output=False,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "show_mask(masks[0], plt.gca())\n",
        "show_box(input_box, plt.gca())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T07:59:51.360799Z",
          "iopub.execute_input": "2024-05-03T07:59:51.361751Z",
          "iopub.status.idle": "2024-05-03T07:59:51.913397Z",
          "shell.execute_reply.started": "2024-05-03T07:59:51.361716Z",
          "shell.execute_reply": "2024-05-03T07:59:51.912508Z"
        },
        "trusted": true,
        "id": "rIyJfDKyOiRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation_mask = masks[0]\n",
        "binary_mask = np.where(segmentation_mask > 0.5, 1, 0)\n",
        "\n",
        "white_background = np.ones_like(image) * 255\n",
        "\n",
        "new_image = white_background * (1 - binary_mask[..., np.newaxis]) + image * binary_mask[..., np.newaxis]\n",
        "\n",
        "plt.imshow(new_image.astype(np.uint8))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:00:53.750135Z",
          "iopub.execute_input": "2024-05-03T08:00:53.750517Z",
          "iopub.status.idle": "2024-05-03T08:00:53.907538Z",
          "shell.execute_reply.started": "2024-05-03T08:00:53.75049Z",
          "shell.execute_reply": "2024-05-03T08:00:53.906568Z"
        },
        "trusted": true,
        "id": "wfPP5K8DOiRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Maybe YOLO"
      ],
      "metadata": {
        "id": "07N2DMMyOiRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the image\n",
        "url = 'https://as1.ftcdn.net/v2/jpg/00/72/21/90/1000_F_72219046_WmTgchiYSNEUWrGggispWfYdT2xLDhbZ.jpg'\n",
        "# Download the image from the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the image to a local file\n",
        "with open('temp_image.jpg', 'wb') as f:\n",
        "    f.write(response.content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:25:18.627379Z",
          "iopub.execute_input": "2024-05-03T08:25:18.62836Z",
          "iopub.status.idle": "2024-05-03T08:25:18.758351Z",
          "shell.execute_reply.started": "2024-05-03T08:25:18.628324Z",
          "shell.execute_reply": "2024-05-03T08:25:18.757612Z"
        },
        "trusted": true,
        "id": "0LmZypZfOiRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('/content/yolov8n.pt')\n",
        "results = model.predict(source='temp_image.jpg', conf=0.25)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:25:21.114709Z",
          "iopub.execute_input": "2024-05-03T08:25:21.115425Z",
          "iopub.status.idle": "2024-05-03T08:25:21.354261Z",
          "shell.execute_reply.started": "2024-05-03T08:25:21.115392Z",
          "shell.execute_reply": "2024-05-03T08:25:21.35331Z"
        },
        "trusted": true,
        "id": "Pc1O2jEDOiRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for result in results:\n",
        "    boxes = result.boxes\n",
        "\n",
        "bbox=boxes.xyxy.tolist()[0]\n",
        "\n",
        "bbox"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:25:30.221897Z",
          "iopub.execute_input": "2024-05-03T08:25:30.222285Z",
          "iopub.status.idle": "2024-05-03T08:25:30.229794Z",
          "shell.execute_reply.started": "2024-05-03T08:25:30.222256Z",
          "shell.execute_reply": "2024-05-03T08:25:30.228835Z"
        },
        "trusted": true,
        "id": "1ZbtPiLuOiRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam_checkpoint = \"/kaggle/working/sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:15:22.702724Z",
          "iopub.execute_input": "2024-05-03T08:15:22.703085Z",
          "iopub.status.idle": "2024-05-03T08:15:29.843958Z",
          "shell.execute_reply.started": "2024-05-03T08:15:22.703061Z",
          "shell.execute_reply": "2024-05-03T08:15:29.842904Z"
        },
        "trusted": true,
        "id": "fkpEFIb2OiRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.cvtColor(cv2.imread('temp_image.jpg'), cv2.COLOR_BGR2RGB)\n",
        "predictor.set_image(image)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:16:27.9718Z",
          "iopub.execute_input": "2024-05-03T08:16:27.97218Z",
          "iopub.status.idle": "2024-05-03T08:17:08.381681Z",
          "shell.execute_reply.started": "2024-05-03T08:16:27.972153Z",
          "shell.execute_reply": "2024-05-03T08:17:08.380561Z"
        },
        "trusted": true,
        "id": "FH4vKHIPOiRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:17:30.03166Z",
          "iopub.execute_input": "2024-05-03T08:17:30.032394Z",
          "iopub.status.idle": "2024-05-03T08:17:30.042669Z",
          "shell.execute_reply.started": "2024-05-03T08:17:30.032361Z",
          "shell.execute_reply": "2024-05-03T08:17:30.041663Z"
        },
        "trusted": true,
        "id": "uXC0XFmGOiRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_box = np.array(bbox)\n",
        "\n",
        "masks, _, _ = predictor.predict(\n",
        "    point_coords=None,\n",
        "    point_labels=None,\n",
        "    box=input_box[None, :],\n",
        "    multimask_output=False,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "show_mask(masks[0], plt.gca())\n",
        "show_box(input_box, plt.gca())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:17:35.02481Z",
          "iopub.execute_input": "2024-05-03T08:17:35.025181Z",
          "iopub.status.idle": "2024-05-03T08:17:35.672466Z",
          "shell.execute_reply.started": "2024-05-03T08:17:35.025152Z",
          "shell.execute_reply": "2024-05-03T08:17:35.671505Z"
        },
        "trusted": true,
        "id": "AUGNQcS_OiRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation_mask = masks[0]\n",
        "binary_mask = np.where(segmentation_mask > 0.5, 1, 0)\n",
        "\n",
        "white_background = np.ones_like(image) * 255\n",
        "\n",
        "new_image = white_background * (1 - binary_mask[..., np.newaxis]) + image * binary_mask[..., np.newaxis]\n",
        "\n",
        "plt.imshow(new_image.astype(np.uint8))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:17:47.521721Z",
          "iopub.execute_input": "2024-05-03T08:17:47.522403Z",
          "iopub.status.idle": "2024-05-03T08:17:47.767292Z",
          "shell.execute_reply.started": "2024-05-03T08:17:47.522369Z",
          "shell.execute_reply": "2024-05-03T08:17:47.76636Z"
        },
        "trusted": true,
        "id": "Bl7B7RIuOiR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier3110 = pipeline(\"image-classification\", model='/kaggle/input/aminfins/checkpoint-3110')\n",
        "image_pil = Image.fromarray(new_image.astype(np.uint8))\n",
        "\n",
        "classifier3110(image_pil)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:24:23.345754Z",
          "iopub.execute_input": "2024-05-03T08:24:23.34654Z",
          "iopub.status.idle": "2024-05-03T08:24:23.58718Z",
          "shell.execute_reply.started": "2024-05-03T08:24:23.346508Z",
          "shell.execute_reply": "2024-05-03T08:24:23.586206Z"
        },
        "trusted": true,
        "id": "-HaBZPNROiR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_pil"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-03T08:24:26.031656Z",
          "iopub.execute_input": "2024-05-03T08:24:26.032403Z",
          "iopub.status.idle": "2024-05-03T08:24:26.126844Z",
          "shell.execute_reply.started": "2024-05-03T08:24:26.032374Z",
          "shell.execute_reply": "2024-05-03T08:24:26.125945Z"
        },
        "trusted": true,
        "id": "Yc4ZH3HvOiR1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}